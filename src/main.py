import logging
from typing import NoReturn

from dotenv import load_dotenv

from classes.VectorStoreFRIDA import VectorStoreFRIDA

# from classes.LLMService import LLMService
# from classes.VectorStoreFRIDA import VectorStoreFRIDA
# from others.frida import get_frida_embeddings
# from others.process_and_add_chunks import run_pipeline  # noqa: F401
# from others.run_search import calculate_cosine_similarity_with_embeddings, run_search
    # run_scraper_separate_files()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
# )

logger = logging.getLogger(__name__)

def main() -> NoReturn:
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ."""
    load_dotenv()

    try:
        vs = VectorStoreFRIDA(table_name="test_novaya")

        before = vs.stats()
        logger.info(f"üìä –î–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {before}")

        vs.index_directory(
            directory="./articles",
            chunk_size=1024,
            chunk_overlap=64,
            batch_size=32,
        )

        after = vs.stats()
        logger.info(f"üìä –ü–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {after}")

        # –ø—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞
        query = "What is the Hubble Space Telescope?"
        results = vs.query(query)
        for r in results:
            logger.info(f"‚≠ê {r['content'][:120]}...")

        # # –≠–¢–ê–ü 2: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞
        # logger.info("‚ñ∂Ô∏è  –≠–¢–ê–ü 2: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞")
        # logger.info("‚îÄ" * 80)

        # test_query = "–ö–∞–∫—É—é —Å–∞–º—É—é –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ–¥–µ—Ä–∂–∏—Ç —ç—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤?"
        # logger.info(f"üìù –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: '{test_query}'\n")

        # # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        # logger.info("üîÑ –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞...")
        # q_emb = get_frida_embeddings(
        #     [test_query],
        #     device="cpu",
        # )

        # # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        # results = vs.search(
        #     q_emb[0],
        #     top_k=5,
        #     match_threshold=0.0,  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        # )

        # # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        # logger.info(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: –Ω–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n")
        # logger.info("‚îÄ" * 80)

        # if results:
        #     for idx, (doc_id, content, metadata, similarity) in enumerate(results, 1):
        #         logger.info(f"\n{idx}. üìÑ –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f}")
        #         logger.info(f"   ID: {doc_id}")
        #         logger.info(f"   –¢–µ–∫—Å—Ç: {content[:100]}...")
        #         if metadata:
        #             logger.info("   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:")
        #             for key, value in metadata.items():
        #                 logger.info(f"      {key}: {value}")
        # else:
        #     logger.warning("‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

        # logger.info("\n" + "=" * 80)
        # logger.info("‚úÖ –ü–†–û–¶–ï–°–° –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
        # logger.info("=" * 80 + "\n")

        return 0

    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 1

    except Exception as e:
        logger.error(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        return 1

    # run_pipeline()

    # vector_store = VectorStoreFRIDA()
    # embedding_function = get_frida_embeddings
    # user_prompt = "Tell me about space exploration on the Moon and Mars."

    # top_text = run_search(user_prompt, vector_store, embedding_function)

    # similarity = calculate_cosine_similarity_with_embeddings(user_prompt, top_text)
    # print(f"Similarity: {similarity}")

    # augmented_input = user_prompt + " " + top_text

    # llm_service = LLMService()

    # start_time = time.time()
    # gpt_response = llm_service.summarize_texts(augmented_input)
    # response_time = time.time() - start_time

    # llm_service.print_formatted_response(gpt_response)
    # print(f"Response Time: {response_time:.2f} seconds")

    # similarity = calculate_cosine_similarity_with_embeddings(user_prompt, gpt_response)
    # print(f"Similarity: {similarity}")


if __name__ == "__main__":
    main()
