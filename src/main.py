import json
import logging
import os
from pathlib import Path
from typing import List, Optional, Tuple

import lancedb
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.schema import Document, TextNode
from llama_index.core.vector_stores.types import VectorStoreQuery, VectorStoreQueryResult
from llama_index.llms.openrouter import OpenRouter
from llama_index.vector_stores.lancedb import LanceDBVectorStore

from others.frida import FridaEmbedding

logging.basicConfig(level=logging.INFO)
load_dotenv()

LANCE_DB_PATH = Path("./lancedb/articles_index").resolve()


class PatchedLanceDBVectorStore(LanceDBVectorStore):
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å LlamaIndex ‚Üî LanceDB –ø—Ä–∏ –≤–æ–∑–≤—Ä–∞—Ç–µ Series."""

    def query(self, query: VectorStoreQuery) -> VectorStoreQueryResult:
        """–ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ query –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ LanceDB
            query_embedding = query.query_embedding
            if query_embedding is None:
                raise ValueError("Query embedding is required")

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if not isinstance(query_embedding, np.ndarray):
                query_embedding = np.array(query_embedding)

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            results = self._table.search(query_embedding).limit(query.similarity_top_k or 10).to_pandas()

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            nodes = []
            similarities = []
            ids = []

            for idx, row in results.iterrows():
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = row.get("metadata", {})
                if isinstance(metadata, str):
                    try:
                        metadata = json.loads(metadata)
                    except json.JSONDecodeError:
                        metadata = {"__node_content__": metadata}
                elif isinstance(metadata, pd.Series):
                    metadata = metadata.to_dict()

                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –µ—Å—Ç—å __node_content__
                text_content = row.get("text", "")
                if "__node_content__" not in metadata:
                    metadata["__node_content__"] = text_content

                # –°–æ–∑–¥–∞–µ–º TextNode
                node = TextNode(text=text_content, metadata=metadata, id_=metadata.get("doc_id", f"doc_{idx}"))

                nodes.append(node)
                similarities.append(float(row.get("_distance", 0.0)))
                ids.append(metadata.get("doc_id", f"doc_{idx}"))

            return VectorStoreQueryResult(nodes=nodes, similarities=similarities, ids=ids)

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –≤ query: {e}")
            return VectorStoreQueryResult(nodes=[], similarities=[], ids=[])


def fill_lance_dataset(
    documents: List[Document],
    db_path: Path = LANCE_DB_PATH,
) -> Tuple[Optional[PatchedLanceDBVectorStore], List[TextNode]]:
    """–°–æ–∑–¥–∞–µ—Ç –∏ –∑–∞–ø–æ–ª–Ω—è–µ—Ç LanceDB –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
    if not documents:
        logging.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
        return None, []

    logging.info(f"–°–æ–∑–¥–∞—ë–º LanceDB –ø–æ –ø—É—Ç–∏: {db_path}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LanceDB
    db = lancedb.connect(db_path)
    table_name = "articles"

    # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ —É–∂–µ –µ—Å—Ç—å ‚Äî —É–¥–∞–ª—è–µ–º (–¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞)
    if table_name in db.table_names():
        db.drop_table(table_name)

    embed_model = FridaEmbedding()
    Settings.embed_model = embed_model

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    table_data = []
    nodes = []

    for i, doc in enumerate(documents):
        text = doc.text.strip()
        if not text:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            continue

        # –ü–æ–ª—É—á–∞–µ–º embedding
        embedding = embed_model._get_text_embedding(text)
        embedding_array = np.array(embedding, dtype=np.float32)

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        doc_id = doc.doc_id or f"doc_{i}"
        metadata = {
            "doc_id": doc_id,
            "__node_content__": text,
            "source": getattr(doc, "extra_info", {}).get("file_path", "unknown"),
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        table_data.append(
            {
                "id": doc_id,
                "text": text,
                "vector": embedding_array.tolist(),
                "metadata": json.dumps(metadata),  # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            }
        )

        # –°–æ–∑–¥–∞–µ–º TextNode
        node = TextNode(text=text, embedding=embedding_array.tolist(), metadata=metadata, id_=doc_id)
        nodes.append(node)

    if not table_data:
        logging.warning("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
        return None, []

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
    table = db.create_table(
        table_name,
        data=table_data,
        mode="overwrite",
    )

    vector_store = PatchedLanceDBVectorStore(table=table)
    logging.info(f"‚úÖ LanceDB —Å–æ–∑–¥–∞–Ω: {len(table_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    return vector_store, nodes


def load_or_fill_lance(
    db_path: Path = LANCE_DB_PATH,
) -> Tuple[Optional[PatchedLanceDBVectorStore], Optional[List[TextNode]]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é."""
    try:
        db = lancedb.connect(db_path)
        table_name = "articles"

        if table_name in db.table_names():
            logging.info(f"üì¶ LanceDB '{table_name}' –Ω–∞–π–¥–µ–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
            table = db.open_table(table_name)
            vector_store = PatchedLanceDBVectorStore(table=table)

            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∞–±–ª–∏—Ü–µ
            logging.info(f"üìú Lance schema: {table.schema}")
            sample_data = table.to_pandas().head(1)
            if not sample_data.empty:
                logging.info(f"üìä –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä–æ–∫–∏: {sample_data.to_dict(orient='records')}")

            return vector_store, None
        else:
            logging.info("üÜï LanceDB –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –∑–∞–Ω–æ–≤–æ...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            articles_dir = Path("articles/")
            if not articles_dir.exists():
                logging.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {articles_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
                return None, None

            documents = SimpleDirectoryReader(str(articles_dir)).load_data()
            if not documents:
                logging.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏!")
                return None, None

            vector_store, nodes = fill_lance_dataset(documents, db_path=db_path)
            return vector_store, nodes

    except Exception as e:
        logging.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å LanceDB: {e}")
        return None, None


def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            logging.error("‚ùå OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è!")
            return

        vector_store, nodes = load_or_fill_lance()

        if vector_store is None:
            logging.error("‚ùå –í–µ–∫—Ç–æ—Ä–Ω—ã–π store –Ω–µ —Å–æ–∑–¥–∞–Ω, –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
            return

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
        embed_model = FridaEmbedding()
        Settings.embed_model = embed_model

        llm = OpenRouter(
            model="z-ai/glm-4.5-air:free",
            max_tokens=512,
            context_window=4096,
            api_key=api_key,
        )
        Settings.llm = llm

        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        index = VectorStoreIndex.from_vector_store(
            vector_store,
            embed_model=embed_model,
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ query engine
        query_engine = index.as_query_engine(
            response_mode="compact",
            verbose=True,
        )

        logging.info("üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")
        response = query_engine.query("–†–∞—Å—Å–∫–∞–∂–∏ –æ —Ç–µ–ª–µ—Å–∫–æ–ø–∞—Ö")
        print("\n" + "=" * 50)
        print("–û–¢–í–ï–¢:")
        print("=" * 50)
        print(str(response))
        print("=" * 50)

    except Exception as e:
        logging.exception(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")


if __name__ == "__main__":
    main()
